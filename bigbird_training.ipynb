{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31a3d5a",
   "metadata": {},
   "source": [
    "# Preprocessing for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947c82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, \\\n",
    "                            ConfusionMatrixDisplay, matthews_corrcoef, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import BigBirdForSequenceClassification, RobertaForSequenceClassification, \\\n",
    "                            Trainer, TrainingArguments, \\\n",
    "                            BatchEncoding, EvalPrediction, AutoTokenizer, BigBirdForMaskedLM\n",
    "\n",
    "%cd /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd2e7a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cdf608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-imperial\n"
     ]
    }
   ],
   "source": [
    "%cd /data-imperial\n",
    "train_data = pd.read_pickle(\"saved/multilabel_text_with_preds.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1195793",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.read_pickle(\"saved/eval_text_with_labels.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c049af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## truncating longer sequences \n",
    "max_token_length = 2048\n",
    "train_data['text'] = train_data.text.apply(lambda x: x if len(x.split()) < max_token_length else \" \".join(x.split()[:max_token_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06074ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigbird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20504358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding to minimum length\n",
    "min_token_length = 704\n",
    "train_data['text'] = train_data.text.apply(lambda x: x if len(x.split()) >= min_token_length \n",
    "                                           else x + ((\" \" + tokenizer.pad_token)*(min_token_length - len(x.split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4000f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting the dataset by length\n",
    "train_data['length'] = train_data.text.apply(lambda x: len(x.split()))\n",
    "train_data = train_data.sort_values('length', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da40a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the dataset\n",
    "labels = 'substance'\n",
    "train_data = pd.DataFrame(train_data.set_index(\"text\").loc[:,labels])\n",
    "train_data = train_data.astype({labels: 'int32'})\n",
    "train_data.columns = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f38af3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_data, open(\"saved/untokenized.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bba6d",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987dd4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-imperial\n"
     ]
    }
   ],
   "source": [
    "%cd /data-imperial/\n",
    "\n",
    "train_data = pickle.load(open('saved/untokenized.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff7e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(train_data, test_size=0.1)\n",
    "\n",
    "train_data = datasets.Dataset.from_pandas(train_data)\n",
    "test_data = datasets.Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24f862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigbird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fbf828a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea5a33cfb514a2198e54c020bd9e609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e84a7de33a45d88b9ad772ae54a740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['text'], padding = False, truncation=True, max_length = 2048)\n",
    "\n",
    "train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\n",
    "test_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0f3df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data-imperial/bigbird/tokenizer_config.json',\n",
       " 'data-imperial/bigbird/special_tokens_map.json',\n",
       " 'data-imperial/bigbird/spiece.model',\n",
       " 'data-imperial/bigbird/added_tokens.json',\n",
       " 'data-imperial/bigbird/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %cd /\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/bigbird-roberta-base', \n",
    "#                                           max_length = 2048,\n",
    "#                                           cache_dir=\"/data-imperial/cache\")\n",
    "# tokenizer.save_pretrained(\"data-imperial/bigbird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819a4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ad12066",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_data, open(\"saved/substance/single_train_ds.pickle\", \"wb\"))\n",
    "pickle.dump(test_data, open(\"saved/substance/single_test_ds.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b49e7c",
   "metadata": {},
   "source": [
    "## Processing and Tokenizing for MLM Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43158cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-imperial\n"
     ]
    }
   ],
   "source": [
    "%cd /data-imperial\n",
    "train_data = pd.read_pickle(\"saved/working_text_with_labels.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61d59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigbird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e267f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## truncating longer sequences \n",
    "max_token_length = 2048\n",
    "train_data['text'] = train_data.text.apply(lambda x: x if len(x.split()) < max_token_length else \" \".join(x.split()[:max_token_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f462bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding to minimum length\n",
    "min_token_length = 704\n",
    "train_data['text'] = train_data.text.apply(lambda x: x if len(x.split()) >= min_token_length \n",
    "                                           else x + ((\" \" + tokenizer.pad_token)*(min_token_length - len(x.split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5bb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_data, test_size=0.1)\n",
    "_, extra_short = train_test_split(train_data, test_size=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2068781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer(text, \n",
    "                   return_tensors='pt', \n",
    "                   max_length=2048, \n",
    "                   truncation=True, \n",
    "                   padding=\"max_length\")\n",
    "\n",
    "train = tokenize(list(train.text))\n",
    "test = tokenize(list(test.text))\n",
    "extra_short = tokenize(list(extra_short.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d620eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['labels'] = train.input_ids.detach().clone()\n",
    "test['labels'] = test.input_ids.detach().clone()\n",
    "extra_short['labels'] = extra_short.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cbb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(inputs): \n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 1) * (inputs.input_ids != 2) * (inputs.input_ids != 0)\n",
    "\n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 67\n",
    "        \n",
    "    return inputs\n",
    "        \n",
    "train = make_mask(train)\n",
    "test = make_mask(test)\n",
    "extra_short = make_mask(extra_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d064e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShoutDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd2c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ShoutDataset(train)\n",
    "test = ShoutDataset(test)\n",
    "extra_short = ShoutDataset(extra_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a7d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train, open(\"saved/mlm/train.pickle\", \"wb\"))\n",
    "pickle.dump(test, open(\"saved/mlm/test.pickle\", \"wb\"))\n",
    "pickle.dump(extra_short, open(\"saved/mlm/short.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091beab3",
   "metadata": {},
   "source": [
    "## Finetuning with MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data-imperial\n",
    "train = pickle.load(open(\"saved/mlm/train.pickle\", \"rb\"))\n",
    "test = pickle.load(open(\"saved/mlm/test.pickle\", \"rb\"))\n",
    "extra_short = pickle.load(open(\"saved/mlm/short.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0cfc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bigbird were not used when initializing BigBirdForMaskedLM: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForMaskedLM were not initialized from the model checkpoint at bigbird and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigBirdForMaskedLM.from_pretrained(\"bigbird\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2913182",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = 'bigbird/output/finetune',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 32,    \n",
    "    per_device_eval_batch_size= 2,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=160,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 4,\n",
    "    learning_rate = 1e-5,\n",
    "    log_level = 'warning', \n",
    "    fp16 = True,\n",
    "    logging_dir='bigbird/logs/finetune',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'bigbird_classification_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e53620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # argmax(pred.predictions, axis=1)\n",
    "    #pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds, labels=[0,1])\n",
    "    auprc = average_precision_score(labels, preds)\n",
    "    print(cm)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'mcc': mcc,\n",
    "        'auprc': auprc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e12324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1597b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiatann\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bigbird_classification_test</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kiatann/huggingface\" target=\"_blank\">https://wandb.ai/kiatann/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kiatann/huggingface/runs/1f6jkubm\" target=\"_blank\">https://wandb.ai/kiatann/huggingface/runs/1f6jkubm</a><br/>\n",
       "                Run data is saved locally in <code>/data-imperial/wandb/run-20210822_015858-1f6jkubm</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3156/1461868730.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/data-imperial/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2410' max='2410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2410/2410 26:48:57, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.143269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.122171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.114133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>0.111010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3156/1461868730.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/data-imperial/lib/python3.9/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/tmp/ipykernel_3156/1461868730.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_3156/1461868730.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/data-imperial/lib/python3.9/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/tmp/ipykernel_3156/1461868730.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2410, training_loss=0.14594357851506268, metrics={'train_runtime': 96583.7383, 'train_samples_per_second': 1.598, 'train_steps_per_second': 0.025, 'total_flos': 1.6361164666935706e+17, 'train_loss': 0.14594357851506268, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fe21f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5abcb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-imperial\n"
     ]
    }
   ],
   "source": [
    "%cd /data-imperial/\n",
    "\n",
    "train_data = pickle.load(open('saved/substance/single_train_ds.pickle', 'rb'))\n",
    "test_data = pickle.load(open('saved/substance/single_test_ds.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3173f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BigBirdForSequenceClassification.from_pretrained('bigbird/output/substance/epoch-5',\n",
    "                                                         gradient_checkpointing=False,\n",
    "                                                         num_labels=2,\n",
    "                                                         cache_dir='/data-imperial/cache',\n",
    "                                                         return_dict=True)\n",
    "\n",
    "# # model.save_pretrained(\"data-imperial/bigbird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15649957",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from checkpoint\n",
    "# checkpoint = \"bigbird/output/ml/epoch-5\" ## edit accordingly\n",
    "# model = BigBirdForSequenceClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bigbird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dde0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # argmax(pred.predictions, axis=1)\n",
    "    #pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds, labels=[0,1])\n",
    "    auprc = average_precision_score(labels, preds)\n",
    "    print(cm)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'mcc': mcc,\n",
    "        'auprc': auprc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab736c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'bigbird/output/substance',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 32,    \n",
    "    per_device_eval_batch_size= 2,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=160,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 4,\n",
    "    learning_rate = 1e-5,\n",
    "    log_level = 'warning', \n",
    "    fp16 = True,\n",
    "    logging_dir='bigbird/logs/substance',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'bigbird_classification_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efa253a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data\n",
    ")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df67e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f884ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiatann\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bigbird_classification_test</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kiatann/huggingface\" target=\"_blank\">https://wandb.ai/kiatann/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kiatann/huggingface/runs/3dj7tjoj\" target=\"_blank\">https://wandb.ai/kiatann/huggingface/runs/3dj7tjoj</a><br/>\n",
       "                Run data is saved locally in <code>/data-imperial/wandb/run-20210825_095416-3dj7tjoj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3855' max='3855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3855/3855 25:55:44, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Auprc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.032949</td>\n",
       "      <td>0.994713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>0.994713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>0.994713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.994348</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.347474</td>\n",
       "      <td>0.126434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.994348</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.347474</td>\n",
       "      <td>0.126434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5456    0]\n",
      " [  29    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data-imperial/lib/python3.9/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:00:37.840137, resuming normal operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5456    0]\n",
      " [  29    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data-imperial/lib/python3.9/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/data-imperial/lib/python3.9/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5456    0]\n",
      " [  29    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data-imperial/lib/python3.9/site-packages/sklearn/metrics/_classification.py:873: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/data-imperial/lib/python3.9/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5446   10]\n",
      " [  21    8]]\n",
      "[[5446   10]\n",
      " [  21    8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3855, training_loss=0.023155309161323516, metrics={'train_runtime': 93375.4023, 'train_samples_per_second': 2.643, 'train_steps_per_second': 0.041, 'total_flos': 2.0114757042812214e+17, 'train_loss': 0.023155309161323516, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6ff89",
   "metadata": {},
   "source": [
    "### BigBird Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d1fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = pickle.load(open(\"saved/working_eval_ds_final.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04bbd06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BigBirdForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3608\n",
      "  Batch size = 32\n",
      "/data-imperial/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113/113 13:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2028  340]\n",
      " [ 149 1091]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiatann\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">roberta_classification_test</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kiatann/huggingface\" target=\"_blank\">https://wandb.ai/kiatann/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kiatann/huggingface/runs/3kyx1y25\" target=\"_blank\">https://wandb.ai/kiatann/huggingface/runs/3kyx1y25</a><br/>\n",
       "                Run data is saved locally in <code>/data-imperial/wandb/run-20210803_090136-3kyx1y25</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3115261495113373,\n",
       " 'eval_accuracy': 0.8644678492239468,\n",
       " 'eval_f1': 0.8169225009359791,\n",
       " 'eval_precision': 0.7624039133473096,\n",
       " 'eval_recall': 0.8798387096774194,\n",
       " 'eval_mcc': 0.7147965832312904,\n",
       " 'eval_auprc': 0.7120895928891416,\n",
       " 'eval_runtime': 787.0118,\n",
       " 'eval_samples_per_second': 4.584,\n",
       " 'eval_steps_per_second': 0.144}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=final_test_data) ## need to compute final_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3378785",
   "metadata": {},
   "source": [
    "## Substance Use Big Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46824a1a",
   "metadata": {},
   "source": [
    "### Epoch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f02c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2743' max='2743' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2743/2743 13:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5446   10]\n",
      " [  21    8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiatann\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bigbird_classification_test</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kiatann/huggingface\" target=\"_blank\">https://wandb.ai/kiatann/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kiatann/huggingface/runs/3etzvwcs\" target=\"_blank\">https://wandb.ai/kiatann/huggingface/runs/3etzvwcs</a><br/>\n",
       "                Run data is saved locally in <code>/data-imperial/wandb/run-20210826_122238-3etzvwcs</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.019612709060311317,\n",
       " 'eval_accuracy': 0.9943482224247949,\n",
       " 'eval_f1': 0.3404255319148936,\n",
       " 'eval_precision': 0.4444444444444444,\n",
       " 'eval_recall': 0.27586206896551724,\n",
       " 'eval_mcc': 0.34747396641953576,\n",
       " 'eval_auprc': 0.12643398750336166,\n",
       " 'eval_runtime': 820.9527,\n",
       " 'eval_samples_per_second': 6.681,\n",
       " 'eval_steps_per_second': 3.341}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107724e3",
   "metadata": {},
   "source": [
    "### Epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f56b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-imperial/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2743' max='2743' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2743/2743 13:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5446   10]\n",
      " [  21    8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkiatann\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bigbird_classification_test</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kiatann/huggingface\" target=\"_blank\">https://wandb.ai/kiatann/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kiatann/huggingface/runs/1wx9c8nd\" target=\"_blank\">https://wandb.ai/kiatann/huggingface/runs/1wx9c8nd</a><br/>\n",
       "                Run data is saved locally in <code>/data-imperial/wandb/run-20210826_123746-1wx9c8nd</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.022831875830888748,\n",
       " 'eval_accuracy': 0.9943482224247949,\n",
       " 'eval_f1': 0.3404255319148936,\n",
       " 'eval_precision': 0.4444444444444444,\n",
       " 'eval_recall': 0.27586206896551724,\n",
       " 'eval_mcc': 0.34747396641953576,\n",
       " 'eval_auprc': 0.12643398750336166,\n",
       " 'eval_runtime': 820.4853,\n",
       " 'eval_samples_per_second': 6.685,\n",
       " 'eval_steps_per_second': 3.343}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d651be5",
   "metadata": {},
   "source": [
    "## Model Summary for BigBird and RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c392c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "BigBirdForSequenceClassification                                  --\n",
       "BigBirdModel: 1-1                                               --\n",
       "    BigBirdEmbeddings: 2-1                                     --\n",
       "        Embedding: 3-1                                        38,674,944\n",
       "        Embedding: 3-2                                        3,145,728\n",
       "        Embedding: 3-3                                        1,536\n",
       "        LayerNorm: 3-4                                        1,536\n",
       "        Dropout: 3-5                                          --\n",
       "    BigBirdEncoder: 2-2                                        --\n",
       "        ModuleList: 3-6                                       85,054,464\n",
       "    Linear: 2-3                                                590,592\n",
       "    Tanh: 2-4                                                  --\n",
       "BigBirdClassificationHead: 1-2                                  --\n",
       "    Linear: 2-5                                                590,592\n",
       "    Dropout: 2-6                                               --\n",
       "    Linear: 2-7                                                1,538\n",
       "==========================================================================================\n",
       "Total params: 128,060,930\n",
       "Trainable params: 128,060,930\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c5b637",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "RobertaForSequenceClassification                             --\n",
       "RobertaModel: 1-1                                          --\n",
       "    RobertaEmbeddings: 2-1                                --\n",
       "        Embedding: 3-1                                   38,603,520\n",
       "        Embedding: 3-2                                   394,752\n",
       "        Embedding: 3-3                                   768\n",
       "        LayerNorm: 3-4                                   1,536\n",
       "        Dropout: 3-5                                     --\n",
       "    RobertaEncoder: 2-2                                   --\n",
       "        ModuleList: 3-6                                  85,054,464\n",
       "RobertaClassificationHead: 1-2                             --\n",
       "    Linear: 2-3                                           590,592\n",
       "    Dropout: 2-4                                          --\n",
       "    Linear: 2-5                                           1,538\n",
       "=====================================================================================\n",
       "Total params: 124,647,170\n",
       "Trainable params: 124,647,170\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
